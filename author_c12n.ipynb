{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 不要な警告を非表示にする\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 著者分類器 (夏目漱石, 森鴎外, 芥川龍之介, 宮沢賢治)\n",
    "\n",
    "青空文庫のデータを訓練データとして、モデルを生成し、ある文がどの著者によるものか、あるいは著者っぽさを判定する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['author', ' \"url\"'],\n",
       " ['natsume',\n",
       "  ' http://www.aozora.gr.jp/index_pages/person148.html#sakuhin_list_1'],\n",
       " ['akutagawa',\n",
       "  ' http://www.aozora.gr.jp/index_pages/person879.html#sakuhin_list_1'],\n",
       " ['mori',\n",
       "  ' http://www.aozora.gr.jp/index_pages/person129.html#sakuhin_list_1'],\n",
       " ['miyazawa',\n",
       "  ' https://www.aozora.gr.jp/index_pages/person81.html#sakuhin_list_1']]"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 必要なライブラリのimport\n",
    "# 最後にCNNを用いてモデルを生成するため、オープンソースニューラスネットワークライブラリであるKerasをimportしている\n",
    "import sys, os.path, re, csv, os, glob\n",
    "import pandas as pds\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import codecs\n",
    "from keras.layers import Activation, Dense, Dropout, Flatten, Convolution2D, MaxPooling2D, Reshape, Input, concatenate\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import LearningRateScheduler, Callback, CSVLogger, ModelCheckpoint\n",
    "\n",
    "# ワーキングディレクトリの設定\n",
    "base_url = \"http://www.aozora.gr.jp/\"\n",
    "data_dir = \"./\"\n",
    "aozora_dir = data_dir + \"aozora_data/\"\n",
    "log_dir = aozora_dir + \"log/\"\n",
    "target_author_file = data_dir + \"target_author.csv\"\n",
    "\n",
    "auth_target = []\n",
    "with open(target_author_file,\"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        auth_target.append(row)\n",
    "\n",
    "# ターゲットとする著者と作品一覧が載っているURLのリスト\n",
    "auth_target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 教師データの生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105372 63536 109323 50871\n"
     ]
    }
   ],
   "source": [
    "# 各著者についてラベル付きの文データを生成する\n",
    "def author_data_integ(auth_target=auth_target):\n",
    "    for w in auth_target[1:]:\n",
    "        print (\"starting: \" + w[0])\n",
    "        auth_dir = '{}{}/'.format(aozora_dir, w[0])\n",
    "        csv_dir = '{}{}'.format(auth_dir, \"csv/\")\n",
    "        files = os.listdir(csv_dir)\n",
    "        integ_np = np.array([[\"author\", \"line\"]])\n",
    "        for file in files:\n",
    "            if \"csv\" in file:\n",
    "                print (\"   now at: \" + file)\n",
    "                file_name = csv_dir + file\n",
    "                pds_data = pds.read_csv(file_name, index_col=0)\n",
    "                pds_data = pds_data.dropna()\n",
    "                np_data = np.array(pds_data.ix[:,[0,2]])\n",
    "\n",
    "                out = [j for j in range(len(np_data)) if '-----------' in str(np_data[j,1])]\n",
    "                if not out: out = [1]\n",
    "                hyphen_pos = int(out[len(out) - 1])\n",
    "\n",
    "                last_20 = len(np_data) - 20\n",
    "\n",
    "                np_data = np_data[hyphen_pos+1:last_20,:]\n",
    "                integ_np = np.vstack((integ_np, np_data))\n",
    "\n",
    "        integ_pds = pds.DataFrame(integ_np[1:,:], columns=integ_np[0,:])\n",
    "        integ_pds.to_csv(auth_dir + w[0] + '_integ.csv', quoting=csv.QUOTE_ALL)\n",
    "        print (\"finished: \" + w[0])\n",
    "        \n",
    "def load_integ(author, auth_dir):\n",
    "    integ_csv = auth_dir + author + \"_integ.csv\"\n",
    "    data = pds.read_csv(integ_csv)\n",
    "    return data\n",
    "\n",
    "natsume_data = load_integ(auth_target[1][0], auth_dir = '{}{}/'.format(aozora_dir, auth_target[1][0]))\n",
    "np_natsume = np.array(natsume_data.ix[1:,1:])\n",
    "\n",
    "akutagawa_data = load_integ(auth_target[2][0], auth_dir = '{}{}/'.format(aozora_dir, auth_target[2][0]))\n",
    "np_akutagawa = np.array(akutagawa_data.ix[1:,1:])\n",
    "\n",
    "mori_data = load_integ(auth_target[3][0], auth_dir = '{}{}/'.format(aozora_dir, auth_target[3][0]))\n",
    "np_mori = np.array(mori_data.ix[1:,1:])\n",
    "\n",
    "miyazawa_data = load_integ(auth_target[4][0], auth_dir = '{}{}/'.format(aozora_dir, auth_target[4][0]))\n",
    "np_miyazawa = np.array(miyazawa_data.ix[1:,1:])\n",
    "\n",
    "natsume_txt = np.array([np_natsume[:,1]]).T\n",
    "akutagawa_txt = np.array([np_akutagawa[:,1]]).T\n",
    "mori_txt = np.array([np_mori[:,1]]).T\n",
    "miyazawa_txt = np.array([np_miyazawa[:,1]]).T\n",
    "\n",
    "natsume_id = np.array([np.zeros(len(np_natsume))]).T\n",
    "akutagawa_id = np.array([np.zeros(len(np_akutagawa)) + 1]).T\n",
    "mori_id = np.array([np.zeros(len(np_mori)) + 2]).T\n",
    "miyazawa_id = np.array([np.zeros(len(np_miyazawa)) + 3]).T\n",
    "\n",
    "natsume = np.hstack((natsume_txt, natsume_id))\n",
    "akutagawa = np.hstack((akutagawa_txt, akutagawa_id))\n",
    "mori = np.hstack((mori_txt, mori_id))\n",
    "miyazawa = np.hstack((miyazawa_txt, miyazawa_id))\n",
    "\n",
    "# サンプル数の比較\n",
    "print (len(natsume), len(akutagawa), len(mori), len(miyazawa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['亡友の記念だと思つて長い間それを袋の中に入れて仕舞つて置いた。', 0.0],\n",
       "       ['年數の經つに伴れて、ある時は丸で袋の所在を忘れて打ち過ぎる事も多かつた。', 0.0],\n",
       "       ['近頃｜不圖思ひ出して、あゝして置いては轉宅の際などに何處へ散逸するかも知れないから、今のうちに表具屋へ遣つて懸物にでも仕立てさせやうと云ふ氣が起つた。',\n",
       "        0.0],\n",
       "       ...,\n",
       "       ['あれは胡瓜を擦ったんです。', 0.0],\n",
       "       ['患者さんが足が熱って仕方がない、胡瓜の汁で冷してくれとおっしゃるもんですから私が始終擦って上げました」', 0.0],\n",
       "       ['「じゃやっぱり大根おろしの音なんだね」', 0.0]], dtype=object)"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "natsume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['その次ぎには、その小説の中に描かれた生活に憧憬を持つてゐる。', 1.0],\n",
       "       ['これには時々不思議な気持がしないことはない。', 1.0],\n",
       "       ['現に僕の知つてゐる或る人などは随分経済的に苦しい暮らしをしてゐながら、富豪や華族ばかり出て来る通俗小説を愛読してゐる。',\n",
       "        1.0],\n",
       "       ...,\n",
       "       ['君の弟が、ステッキをふりまわして「兄さん万歳」を連叫する。', 1.0],\n",
       "       ['――それが、いよいよ、君が全く見えなくなるまで、続いた。', 1.0],\n",
       "       ['帰りぎわに、ふりむいて見たら、例の年よりの異人は、まだ、ぼんやり船の出て行った方をながめている。', 1.0]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "akutagawa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['どうかして間違って二度話し掛けて、その客に「ひゅうひゅうと云うのだろう」なんぞと、先を越して云われようものなら、お金の悔やしがりようは一通りではない。',\n",
       "        2.0],\n",
       "       ['なぜと云うに、あの女は一度来た客を忘れると云うことはないと云って、ひどく自分の記憶を恃んでいたからである。', 2.0],\n",
       "       ['それを客の方から頼んで二度話して貰ったものは、恐らくは僕一人であろう。', 2.0],\n",
       "       ...,\n",
       "       ['住いは六十五のとき下谷徒士町に移り、六十七のとき一時藩の上邸に入っていて、麹町一丁目半蔵門外の壕端の家を買って移った。',\n",
       "        2.0],\n",
       "       ['策士｜雲井龍雄と月見をした海嶽楼は、この家の二階である。', 2.0],\n",
       "       ['幕府滅亡の余波で、江戸の騒がしかった年に、仲平は七十で表向き隠居した。', 2.0]], dtype=object)"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['荒き渚のあけがたを', 3.0],\n",
       "       ['家長は白きもんぱして', 3.0],\n",
       "       ['こらをはげまし急ぎくる', 3.0],\n",
       "       ...,\n",
       "       ['※〔〕付きの表題は、底本編集時におぎなわれたものです。', 3.0],\n",
       "       ['入力：junk', 3.0],\n",
       "       ['校正：土屋隆', 3.0]], dtype=object)"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "miyazawa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ngramによる言語モデルの生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_integ = np.vstack((np.vstack((np.vstack((natsume, akutagawa)),mori)),miyazawa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ngram\n",
    "\n",
    "# ngramと全文検索を用いた分類器\n",
    "def ngram_search_c12n(raw_txt):\n",
    "    all_data = [natsume, mori, akutagawa, miyazawa]\n",
    "    target_author = [\"夏目漱石\",\"森鴎外\",\"芥川龍之介\",\"宮沢賢治\"]\n",
    "    score_list = []\n",
    "    for data in all_data:\n",
    "        G = ngram.NGram(data[:,0].tolist())\n",
    "        score_list.append(G.search(raw_txt)[0][1])\n",
    "    return target_author[score_list.index(max(score_list))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'宮沢賢治'"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_txt = \"\"\"\n",
    "もうけつしてさびしくはない\n",
    "なんべんさびしくないと云つたとこで\n",
    "またさびしくなるのはきまつてゐる\n",
    "\"\"\"\n",
    "ngram_search_c12n(raw_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'宮沢賢治'"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_txt = \"\"\"\n",
    "ぼんやりとした不安\n",
    "\"\"\"\n",
    "ngram_search_c12n(raw_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'夏目漱石'"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_txt = \"\"\"\n",
    "のどかな春の日を\n",
    "\n",
    "鳴き尽くし、鳴きあかし、\n",
    "\n",
    "また鳴き暮らさなければ気が済まんと見える。\n",
    "\n",
    "その上どこまでも登って行く、\n",
    "\n",
    "いつまでも登って行く。\n",
    "\n",
    "雲雀はきっと雲の中で死ぬに相違ない。\n",
    "\n",
    "登り詰めた揚句は、\n",
    "\n",
    "流れて雲に入って、\n",
    "\n",
    "漂うているうちに形は消えてなくなって、\n",
    "\n",
    "ただ声だけが空の裡に残るのかもしれない。\n",
    "\"\"\"\n",
    "ngram_search_c12n(raw_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'森鴎外'"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_txt = \"\"\"\n",
    "ロシアのビザめんどくさいねえ\n",
    "\"\"\"\n",
    "ngram_search_c12n(raw_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['亡友の記念だと思つて長い間それを袋の中に入れて仕舞つて置いた。',\n",
       "       '年數の經つに伴れて、ある時は丸で袋の所在を忘れて打ち過ぎる事も多かつた。',\n",
       "       '近頃｜不圖思ひ出して、あゝして置いては轉宅の際などに何處へ散逸するかも知れないから、今のうちに表具屋へ遣つて懸物にでも仕立てさせやうと云ふ氣が起つた。',\n",
       "       ..., '※〔〕付きの表題は、底本編集時におぎなわれたものです。', '入力：junk', '校正：土屋隆'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_integ[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['亡友の記念だと思つて長い間それを袋の中に入れて仕舞つて置いた。',\n",
       "       '年數の經つに伴れて、ある時は丸で袋の所在を忘れて打ち過ぎる事も多かつた。',\n",
       "       '近頃｜不圖思ひ出して、あゝして置いては轉宅の際などに何處へ散逸するかも知れないから、今のうちに表具屋へ遣つて懸物にでも仕立てさせやうと云ふ氣が起つた。',\n",
       "       ..., 'あれは胡瓜を擦ったんです。',\n",
       "       '患者さんが足が熱って仕方がない、胡瓜の汁で冷してくれとおっしゃるもんですから私が始終擦って上げました」',\n",
       "       '「じゃやっぱり大根おろしの音なんだね」'], dtype=object)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "natsume[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MeCab\n",
    "mecab = MeCab.Tagger(\"-Owakati\")\n",
    "\n",
    "def mecab_parse(s):\n",
    "    return mecab.parse(str(s)).strip().split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNNによる学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_integ = np.vstack((np.vstack((np.vstack((natsume, akutagawa)),mori)),miyazawa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(txt, max_length=200):\n",
    "    txt_list = []\n",
    "    for l in txt:\n",
    "        txt_line = [ord(x) for x in str(l).strip()]\n",
    "        # You will get encoded text in array, just like this\n",
    "        # [25991, 31456, 12391, 12399, 12394, 12367, 12387, 12390, 23383, 24341, 12391, 12354, 12427, 12290]\n",
    "        txt_line = txt_line[:max_length]\n",
    "        txt_len = len(txt_line)\n",
    "        if txt_len < max_length:\n",
    "            txt_line += ([0] * (max_length - txt_len))\n",
    "        txt_list.append((txt_line))\n",
    "    return txt_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_list = load_data(txt = data_integ[:,0])\n",
    "np_txt = np.array(txt_list)\n",
    "\n",
    "tgt_list = data_integ[:,1]\n",
    "np_tgt_list = np_utils.to_categorical(tgt_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[20129, 21451, 12398, ...,     0,     0,     0],\n",
       "       [24180, 25976, 12398, ...,     0,     0,     0],\n",
       "       [36817, 38915, 65372, ...,     0,     0,     0],\n",
       "       ...,\n",
       "       [ 8251, 12308, 12309, ...,     0,     0,     0],\n",
       "       [20837, 21147, 65306, ...,     0,     0,     0],\n",
       "       [26657, 27491, 65306, ...,     0,     0,     0]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0, 0.0, 0.0, ..., 3.0, 3.0, 3.0], dtype=object)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "329102"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np_tgt_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "329102"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_list = load_data(txt = data_integ[:,0])\n",
    "np_txt = np.array(txt_list)\n",
    "\n",
    "tgt_list = data_integ[:,1]\n",
    "np_tgt_list = np_utils.to_categorical(tgt_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_15 (InputLayer)           (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_15 (Embedding)        (None, 200, 128)     8388480     input_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_17 (Reshape)            (None, 200, 128, 1)  0           embedding_15[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_56 (Conv2D)              (None, 199, 1, 64)   16448       reshape_17[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (None, 198, 1, 64)   24640       reshape_17[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_58 (Conv2D)              (None, 197, 1, 64)   32832       reshape_17[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_59 (Conv2D)              (None, 196, 1, 64)   41024       reshape_17[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_56 (MaxPooling2D) (None, 1, 1, 64)     0           conv2d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_57 (MaxPooling2D) (None, 1, 1, 64)     0           conv2d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_58 (MaxPooling2D) (None, 1, 1, 64)     0           conv2d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_59 (MaxPooling2D) (None, 1, 1, 64)     0           conv2d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 1, 1, 256)    0           max_pooling2d_56[0][0]           \n",
      "                                                                 max_pooling2d_57[0][0]           \n",
      "                                                                 max_pooling2d_58[0][0]           \n",
      "                                                                 max_pooling2d_59[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "reshape_18 (Reshape)            (None, 256)          0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 64)           16448       reshape_18[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 64)           256         dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 64)           0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 4)            260         dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 8,520,388\n",
      "Trainable params: 8,520,260\n",
      "Non-trainable params: 128\n",
      "__________________________________________________________________________________________________\n",
      "Train on 296191 samples, validate on 32911 samples\n",
      "Epoch 1/4\n",
      "296191/296191 [==============================] - 970s 3ms/step - loss: 0.2620 - acc: 0.8982 - val_loss: 1.6109 - val_acc: 0.5584\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.55838, saving model to /tmp/weights.01-0.26-0.90-1.61-0.56.hdf5\n",
      "Epoch 2/4\n",
      "296191/296191 [==============================] - 965s 3ms/step - loss: 0.2112 - acc: 0.9172 - val_loss: 1.6674 - val_acc: 0.5560\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.55838\n",
      "Epoch 3/4\n",
      "296191/296191 [==============================] - 969s 3ms/step - loss: 0.1626 - acc: 0.9355 - val_loss: 1.3060 - val_acc: 0.6446\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.55838 to 0.64459, saving model to /tmp/weights.03-0.16-0.94-1.31-0.64.hdf5\n",
      "Epoch 4/4\n",
      "296191/296191 [==============================] - 952s 3ms/step - loss: 0.1324 - acc: 0.9469 - val_loss: 1.5814 - val_acc: 0.6041\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.64459\n"
     ]
    }
   ],
   "source": [
    "def create_model(embed_size=128, max_length=200, filter_sizes=(2, 3, 4, 5), filter_num=64):\n",
    "    inp = Input(shape=(max_length,))\n",
    "    emb = Embedding(0xffff, embed_size)(inp)\n",
    "    emb_ex = Reshape((max_length, embed_size, 1))(emb)\n",
    "    \n",
    "    convs = []\n",
    "    for filter_size in filter_sizes:\n",
    "        # 畳み込み層\n",
    "        conv = Convolution2D(filter_num, filter_size, embed_size, activation=\"relu\")(emb_ex)\n",
    "        # プーリング層\n",
    "        pool = MaxPooling2D(pool_size=(max_length - filter_size + 1, 1))(conv)\n",
    "        convs.append(pool)\n",
    "    # concatenateで層をマージ\n",
    "    convs_merged = concatenate(convs)\n",
    "    reshape = Reshape((filter_num * len(filter_sizes),))(convs_merged)\n",
    "    fc1 = Dense(64, activation=\"relu\")(reshape) # 全結合ニューラルネットワークレイヤー(64次元)、ReLU関数\n",
    "    bn1 = BatchNormalization()(fc1) # 隠れ層に対する標準化\n",
    "    do1 = Dropout(0.5)(bn1) # ドロップアウト（過学習を避ける）\n",
    "    fc2 = Dense(4, activation='sigmoid')(do1) # 全結合ニューラルネットワークレイヤー(4次元)、シグモイド関数\n",
    "    model = Model(input=inp, output=fc2)# モデルの生成\n",
    "    return model\n",
    "\n",
    "def train(inputs, targets, batch_size=100, epoch_count=4, max_length=200, model_filepath=aozora_dir + \"model.h5\", learning_rate=0.001):\n",
    "    start = learning_rate #学習率\n",
    "    stop = learning_rate * 0.01\n",
    "    learning_rates = np.linspace(start, stop, epoch_count) # 学習率の等差数列\n",
    "\n",
    "    model = create_model(max_length=max_length) # モデルを生成\n",
    "    optimizer = Adam(lr=learning_rate) # アダムオプティマイザによる最適化\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])  # モデルのコンパイル\n",
    "    model.summary()\n",
    "    \n",
    "    # モデルをファイルとして保存しておく\n",
    "    target = os.path.join('/tmp', 'weights.*.hdf5')\n",
    "    files = [(f, os.path.getmtime(f)) for f in glob.glob(target)]\n",
    "    if len(files) != 0:\n",
    "        latest_saved_model = sorted(files, key=lambda files: files[1])[-1]\n",
    "        model.load_weights(latest_saved_model[0])\n",
    "    csv_logger_file = '/tmp/clcnn_training.log'\n",
    "    checkpoint_filepath = \"/tmp/weights.{epoch:02d}-{loss:.2f}-{acc:.2f}-{val_loss:.2f}-{val_acc:.2f}.hdf5\"\n",
    "\n",
    "    model.fit(inputs, targets,\n",
    "              nb_epoch=epoch_count,\n",
    "              batch_size=batch_size,\n",
    "              verbose=1,\n",
    "              validation_split=0.1,\n",
    "              shuffle=True,\n",
    "              callbacks=[\n",
    "                  LearningRateScheduler(lambda epoch: learning_rates[epoch]),\n",
    "                  CSVLogger(csv_logger_file),\n",
    "                  ModelCheckpoint(filepath=checkpoint_filepath, verbose=1, save_best_only=True, save_weights_only=False, monitor='val_acc')\n",
    "              ]) # モデルを学習させる、inputs、targetsがそれぞれデータとラベル\n",
    "\n",
    "    model.save(model_filepath) # モデルをファイルに保存\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train(np_txt, np_tgt_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pds\n",
    "from keras.models import load_model\n",
    "\n",
    "model_file = \"/tmp/weights.01-0.26-0.90-1.61-0.56.hdf5\"\n",
    "target_author = [\"夏目漱石\",\"芥川龍之介\",\"森鴎外\",\"宮沢賢治\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encodes the raw_txt\n",
    "def text_encoding(raw_txt):\n",
    "    txt = [ord(x) for x in str(raw_txt).strip()]\n",
    "    txt = txt[:200]\n",
    "    if len(txt) < 200:\n",
    "        txt += ([0] * (200 - len(txt)))\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "def predict(comments, model_filepath=\"tmp/model.h5\"):\n",
    "    model = load_model(model_filepath)\n",
    "    ret = model.predict(comments)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def result(raw_txt):\n",
    "    txt = text_encoding(raw_txt)\n",
    "    predict_result = predict(np.array([txt]), model_filepath=model_file)\n",
    "    pds_predict_result = pds.DataFrame(predict_result, columns=target_author)\n",
    "    print(pds_predict_result)\n",
    "    return target_author[np.argmax(pds_predict_result.as_matrix())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      夏目漱石     芥川龍之介       森鴎外          宮沢賢治\n",
      "0  0.00001  0.000025  0.001205  4.801459e-08\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'森鴎外'"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_txt = \"隴西の李徴は博學才穎、天寶の末年、若くして名を虎榜に連ね、ついで江南尉に補せられたが、性、狷介、自ら恃む所頗る厚く、賤吏に甘んずるを潔しとしなかつた。\"\n",
    "result(raw_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       夏目漱石     芥川龍之介       森鴎外      宮沢賢治\n",
      "0  0.116199  0.000063  0.000002  0.000007\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'夏目漱石'"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_txt = \"\"\"\n",
    "のどかな春の日を\n",
    "\n",
    "鳴き尽くし、鳴きあかし、\n",
    "\n",
    "また鳴き暮らさなければ気が済まんと見える。\n",
    "\n",
    "その上どこまでも登って行く、\n",
    "\n",
    "いつまでも登って行く。\n",
    "\n",
    "雲雀はきっと雲の中で死ぬに相違ない。\n",
    "\n",
    "登り詰めた揚句は、\n",
    "\n",
    "流れて雲に入って、\n",
    "\n",
    "漂うているうちに形は消えてなくなって、\n",
    "\n",
    "ただ声だけが空の裡に残るのかもしれない。\n",
    "\"\"\"\n",
    "result(raw_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       夏目漱石     芥川龍之介       森鴎外      宮沢賢治\n",
      "0  0.000086  0.000445  0.000222  0.000005\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'芥川龍之介'"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_txt = \"\"\"\n",
    "苦難が大きすぎて、\n",
    "自分ひとりの力で支え切れない場合には、\n",
    "家族から身を隠して一人で泣きなさい。\n",
    "\n",
    "そして、苦悩を涙とともに洗い流したら、\n",
    "頭をあげて胸を張り、\n",
    "家族を激励するために家に戻りなさい。\n",
    "\"\"\"\n",
    "result(raw_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       夏目漱石     芥川龍之介       森鴎外      宮沢賢治\n",
      "0  0.000048  0.000001  0.000019  0.000005\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'夏目漱石'"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_txt = \"\"\"\n",
    "それがしの宮の催したまいし星が岡茶寮のドイツ会に、洋行がえりの将校次をおうて身の上ばなしせしときのことなりしが、こよいはおん身が物語聞くべきはずなり、殿下も待ちかねておわすればとうながされて、まだ大尉になりてほどもあらじと見ゆる小林という少年士官、口にくわえし巻煙草まきたばこ取りて火鉢ひばちの中へ灰ふり落して語りははじめぬ。\n",
    "\"\"\"\n",
    "result(raw_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       夏目漱石     芥川龍之介       森鴎外      宮沢賢治\n",
      "0  0.000001  0.000318  0.007525  0.000015\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'森鴎外'"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_txt = \"\"\"\n",
    "九月はじめの秋の空は、きょうしもここにまれなるあい色になりて、空気透すきとおりたれば、残るくまなくあざやかに見ゆるこの群れの真中まなかに、馬車一輛とどめさせて、年若き貴婦人いくたりか乗りたれば、さまざまの衣の色相映じて、花一叢そう、にしき一団、目もあやに、立ちたる人の腰帯シェルペ、坐りたる人の帽のひもなどを、風ひらひらと吹きなびかしたり。そのかたわらに馬立てたる白髪の翁おきなは角つのボタンどめにせし緑の猟人服かりうどふくに、うすき褐かちいろの帽をいただけるのみなれど、なにとなく由よしありげに見ゆ。すこし引き下がりて白き駒こま控えたる少女、わが目がねはしばしこれにとどまりぬ。鋼鉄はがねいろの馬のり衣ごろも裾長すそながに着て、白き薄絹巻きたる黒帽子をかぶりたる身の構えけだかく、いまかなたの森蔭より、むらむらと打ち出でたる猟兵の勇ましさ見んとて、人々騒げどかえりみぬさま心憎し。\n",
    "「殊ことなるかたに心とどめたもうものかな」といいて軽くわが肩をうちし長き八字髭ひげの明色なる少年士官は、おなじ大隊の本部につけられたる中尉にて、男爵フォン、メエルハイムという人なり。「かしこなるはわが識れるデウベンの城のぬしビュロオ伯が一族なり。本部のこよいの宿はかの城と定まりたれば、君も人々に交わりたもうたつきあらん」といいおわるとき、猟兵ようようわが左翼に迫るを見て、メエルハイムは駈け去りぬ。この人とわが交わりそめしは、まだ久しからぬほどなれど、よき性さがとおもわれぬ。\n",
    "\"\"\"\n",
    "result(raw_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       夏目漱石     芥川龍之介       森鴎外      宮沢賢治\n",
      "0  0.001286  0.001987  0.000394  0.000179\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'芥川龍之介'"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_txt = \"\"\"\n",
    "ぼんやりとした不安\n",
    "\"\"\"\n",
    "result(raw_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           夏目漱石     芥川龍之介       森鴎外      宮沢賢治\n",
      "0  9.799999e-07  0.004331  0.000437  0.202875\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'宮沢賢治'"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_txt = \"\"\"\n",
    "もうけつしてさびしくはない\n",
    "なんべんさびしくないと云つたとこで\n",
    "またさびしくなるのはきまつてゐる\n",
    "\"\"\"\n",
    "result(raw_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 参考文献\n",
    "\n",
    "#### Ngramについて\n",
    "- http://www.denzow.me/entry/2017/10/29/121017\n",
    "\n",
    "#### CNN、青空データの扱いについて\n",
    "※このテーマで取り組む途中でかなり近いテーマの記事が見つかってしまったので、CNNではこれを参考にして実装している。\n",
    "- https://qiita.com/bokeneko/items/c0f0ce60a998304400c8\n",
    "- https://qiita.com/cvusk/items/c1342dd0fff16dc37ddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
